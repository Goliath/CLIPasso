{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLIPSketch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnAMiNRNGT0KKL1qZWcKz+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yael-vinker/CLIPSketch/blob/main/CLIPSketch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *CLIPSketch* - sketch your own image"
      ],
      "metadata": {
        "id": "Ht4wCUlzwi18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies and Clone the Repo\n",
        "\n",
        "Make sure your Hardware accelerator is set to GPU.\n",
        "\n",
        "Runtime > Change runtime type > Hardware Accelerator "
      ],
      "metadata": {
        "id": "1V-3h8-awFYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/yael-vinker/CLIPSketch.git\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "user = getpass('GitHub user')\n",
        "password = getpass('GitHub password')\n",
        "os.environ['GITHUB_AUTH'] = user + ':' + password\n",
        "\n",
        "!git clone https://$GITHUB_AUTH@github.com/yael-vinker/CLIPSketch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0mW2URSK_Nd",
        "outputId": "0bfc8aae-978f-483e-e474-2f550a2475cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GitHub user··········\n",
            "GitHub password··········\n",
            "fatal: destination path 'CLIPSketch' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd CLIPSketch\n",
        "!pip install -r requirements.txt\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/BachiLi/diffvg\n",
        "%cd diffvg\n",
        "!git submodule update --init --recursive\n",
        "!python setup.py install"
      ],
      "metadata": {
        "id": "euQD0HH5LylZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/CLIPSketch\n",
        "\n",
        "import subprocess as sp\n",
        "import numpy as np\n",
        "import os\n",
        "import multiprocessing as mp\n",
        "import warnings\n",
        "import pydiffvg\n",
        "import torch\n",
        "\n",
        "from torch.nn.parallel import parallel_apply\n",
        "from shutil import copyfile\n",
        "from PIL import Image\n",
        "\n",
        "#Make sure your Hardware accelerator is set to GPU.\n",
        "#Runtime > Change runtime type > Hardware Accelerator\n",
        "print(torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if (torch.cuda.is_available() and torch.cuda.device_count() > 0) else \"cpu\")\n",
        "print(device)\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter('ignore')\n",
        "warnings.filterwarnings(action='once')\n",
        "\n",
        "manager = mp.Manager()\n",
        "exit_codes = []\n",
        "losses_all = manager.dict()\n",
        "\n",
        "if not os.path.isfile(\"/content/CLIPSketch/U2Net_/saved_models/u2net.pth\"):\n",
        "    sp.run([\"gdown\", \"https://drive.google.com/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ\", \"-O\", \"U2Net_/saved_models/\"])\n",
        "\n",
        "test_name = \"camel\"\n",
        "output_dir = f\"/content/CLIPSketch/output_sketches/{test_name}/\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgNpuzBBTSVa",
        "outputId": "52600c5e-123b-4be4-81d1-f010ddea47c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CLIPSketch\n",
            "True\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run(target, seed, wandb_name):\n",
        "    proc = sp.Popen([\"python\", \"-W\",\"ignore\", \"painterly_rendering.py\", target, \n",
        "                                \"--output_dir\", output_dir,\n",
        "                                \"--wandb_name\", wandb_name,\n",
        "                                \"--num_iter\", str(num_iter),\n",
        "                                \"--save_interval\", str(save_interval),\n",
        "                                \"--seed\", str(seed),\n",
        "                                \"--use_gpu\", str(use_gpu),\n",
        "                                \"--fix_scale\", str(fix_scale), \n",
        "                                \"--mask_object\", str(mask_object),\n",
        "                                \"--mask_object_attention\", str(mask_object_attention)])\n",
        "    try:\n",
        "      outs, errs = proc.communicate(timeout=20000)\n",
        "    except Exception as e:\n",
        "      logging.error(traceback.format_exc())\n",
        "    config = np.load(f\"{output_dir}/{wandb_name}/config.npy\", allow_pickle=True)[()]\n",
        "    loss_eval = np.array(config['loss_eval'])\n",
        "    inds = np.argsort(loss_eval)\n",
        "    losses_all[wandb_name] = loss_eval[inds][0]"
      ],
      "metadata": {
        "id": "D3BYG2ujrV5Z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/CLIPSketch/\n",
        "import traceback\n",
        "import logging\n",
        "targets = [\"/content/CLIPSketch/target_images/camel.png\"]\n",
        "target_names = [\"camel\"]\n",
        "num_iter = 20\n",
        "save_interval = 5\n",
        "use_gpu=1\n",
        "seeds = [100]\n",
        "\n",
        "# if you need to mask the input image and pad the aspect ratio\n",
        "fix_scale=0\n",
        "mask_object=0\n",
        "mask_object_attention=0\n",
        "\n",
        "ncpus=10\n",
        "P = mp.Pool(ncpus) # Generate pool of workers\n",
        "for target, target_name in zip(targets, target_names): # Generate processes\n",
        "    for seed in seeds:\n",
        "        wandb_name=f\"{target_name}_seed{seed}\"\n",
        "        # p = sp.Popen('/bin/bash')\n",
        "        # !python -W ignore painterly_rendering.py \\\n",
        "        # target\n",
        "        P.apply_async(run,(target, seed, wandb_name)) # run simulation and ISF analysis in each process\n",
        "        # run(target, seed, wandb_name)\n",
        "\n",
        "    P.close()\n",
        "    P.join() # start processes \n",
        "\n",
        "    sorted_final = dict(sorted(losses_all.items(), key=lambda item: item[1]))\n",
        "    copyfile(f\"{output_dir}/{list(sorted_final.keys())[0]}/best_iter.svg\", f\"{output_dir}/{list(sorted_final.keys())[0]}_best.svg\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm7LBVl7U4Vh",
        "outputId": "821ba7ea-52c3-4aea-d91a-e9ee2d2c5315"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CLIPSketch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "[W accumulate_grad.h:170] Warning: grad and param do not obey the gradient layout contract. This is not an error, but may impair performance.\n",
            "grad.sizes() = [2048, 512, 1, 1], strides() = [512, 1, 1, 1]\n",
            "param.sizes() = [2048, 512, 1, 1], strides() = [512, 1, 512, 512] (function operator())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results will be saved to /content/CLIPSketch/output_sketches/camel/camel_seed100\n",
            "cuda\n",
            "====================================================================================================\n",
            "test epoch[0/20] loss[0.317962646484375] time[0.37354564666748047]\n",
            "====================================================================================================\n",
            "====================================================================================================\n",
            "test epoch[10/20] loss[0.30161285400390625] time[0.26520490646362305]\n",
            "====================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/CLIPSketch/\n",
        "!python general_test.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtiEiTO8Z_9B",
        "outputId": "190eef9e-6ca7-4b44-b548-e393d07af732"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CLIPSketch\n",
            "in main\n",
            "results will be saved to output_sketches/camel/camel_seed0\n",
            "cuda\n",
            "{'target': 'target_images/camel.png', 'output_dir': 'output_sketches/camel/camel_seed0', 'path_svg': 'none', 'use_gpu': 1, 'seed': 0, 'mask_object': 0, 'fix_scale': 0, 'use_wandb': 0, 'wandb_user': 'yael-vinker', 'wandb_name': 'camel_seed0', 'wandb_project_name': 'none', 'num_iter': 2, 'num_stages': 1, 'lr_scheduler': 0, 'lr': 1.0, 'color_lr': 0.01, 'color_vars_threshold': 0.0, 'batch_size': 1, 'save_interval': 1, 'eval_interval': 10, 'image_scale': 224, 'num_paths': 16, 'width': 1.5, 'control_points_per_seg': 4, 'num_segments': 1, 'attention_init': 1, 'saliency_model': 'clip', 'saliency_clip_model': 'ViT-B/32', 'xdog_intersec': 1, 'mask_object_attention': 0, 'softmax_temp': 0.3, 'percep_loss': 'none', 'perceptual_weight': 0, 'train_with_clip': 0, 'clip_weight': 0, 'start_clip': 0, 'num_aug_clip': 4, 'include_target_in_aug': 0, 'augment_both': 1, 'augemntations': 'affine', 'noise_thresh': 0.5, 'aug_scale_min': 0.7, 'force_sparse': 0, 'clip_conv_loss': 1, 'clip_conv_loss_type': 'L2', 'clip_conv_layer_weights': [0.0, 0.0, 1.0, 1.0, 0.0], 'clip_model_name': 'RN101', 'clip_fc_loss_weight': 0.1, 'clip_text_guide': 0, 'text_target': 'none', 'device': device(type='cuda')}\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2952: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional_tensor.py:876: UserWarning: Argument fill/fillcolor is not supported for Tensor input. Fill value is zero\n",
            "  warnings.warn(\"Argument fill/fillcolor is not supported for Tensor input. Fill value is zero\")\n",
            "====================================================================================================\n",
            "test epoch[0/2] loss[0.29486083984375] time[0.35517334938049316]\n",
            "====================================================================================================\n",
            "[W accumulate_grad.h:170] Warning: grad and param do not obey the gradient layout contract. This is not an error, but may impair performance.\n",
            "grad.sizes() = [2048, 512, 1, 1], strides() = [512, 1, 1, 1]\n",
            "param.sizes() = [2048, 512, 1, 1], strides() = [512, 1, 512, 512] (function operator())\n",
            "==================\n",
            "multi process 26.934223413467407\n"
          ]
        }
      ]
    }
  ]
}